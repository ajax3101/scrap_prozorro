import csv
import json
import os
import time
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
from datetime import datetime
#from config import CODE_EDRPOU as code_e

load_dotenv()

#code_e = os.getenv("U_CODE_EDRPOU")


def read_csv_file(file_path):
    edrpo_data = []

    with open(file_path, 'r', encoding='utf-8') as csvfile:
        csv_reader = csv.reader(csvfile, delimiter=';')

        for row in csv_reader:
            edrpo_data.append(row[2].zfill(8))

    return edrpo_data


def get_all_pages():
    headers = {
        "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
    }
    edrpo_data = read_csv_file('edrpo.csv')
    total_iterations = len(edrpo_data)
    print(f"Количество итераций: {total_iterations}")
    for j, code_e in enumerate(edrpo_data, start=1):
        
        folder_path = os.path.join("data", str(code_e))
        
        # Проверяем, существует ли папка, и если да, пропускаем итерацию
        if os.path.exists(folder_path):
            print(f"[INFO] Итерация {j}/{total_iterations}, Папка уже существует. Пропускаем.")
            continue

        os.makedirs(folder_path, exist_ok=True)
        print(f"[INFO_EDRPO] Качаем ЕДРПО: {code_e} / Осталось: {total_iterations - j}")
        r = requests.get(url=f'https://www.dzo.com.ua/tenders/current?&filter%5Bidentifiers%5D={code_e}', headers=headers)
        
        with open(os.path.join(folder_path, "page_1.html"), "w", encoding="utf-8") as file:
            file.write(r.text)

        with open(os.path.join(folder_path, "page_1.html"), encoding="utf-8") as file:
            src = file.read()
        
        soup = BeautifulSoup(src, 'lxml')
        
        # pages_count = int(soup.find('div', class_="pages relative clear").find_all('a')[-1].text)
        # print(pages_count)
        pages_count_element = soup.find('div', class_="pages relative clear")

        if pages_count_element:
            pages_count = int(pages_count_element.find_all('a')[-1].text)
        else:
            # Если блок не найден, устанавливаем значение по умолчанию
            pages_count = 1

        for i in range(1, pages_count + 1):
            url = f"https://www.dzo.com.ua/tenders/current?filter%5Bidentifiers%5D={code_e}&page={i}"
            #print(url)
            r = requests.get(url=url, headers=headers)
            with open (os.path.join(folder_path, f"page_{i}.html"), "w", encoding="utf-8") as file:
                file.write(r.text)
            time.sleep(5)
            # print(f"[INFO] Скачено страниц {i}/{pages_count}")
            print(f"[INFO] Итерация {j}/{total_iterations}, Скачено страниц {i}/{pages_count}")
            
    print("[INFO] Все итерации завершены")
    # return pages_count + 1 


def collect_data(pages_count):
    cur_date = datetime.now().strftime("%d_%m_%Y")
    
    csv_filename = f"data_{cur_date}.csv"
    json_filename = f"data_{cur_date}.json"

    with open(csv_filename, "w", encoding="utf-8") as csv_file:
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(
            (
                "Tender",
                "URL",
                "CPV",
                "Очікувана вартість закупівлі",
                "Дата оголошення процедури",
                "Поточний статус процедури",
                "Ідентифікатор закупівлі",
                "Організатор закупівлі",
                "Процедура закупівлі",
                "edr_id"  # Новый столбец
            )
        )

    data = []
        
    for page in range(1, pages_count + 1):
        folder_path = os.path.join("data", str(code_e))
        file_path = os.path.join(folder_path, f"page_{page}.html")

        if not os.path.exists(file_path):
            print(f"[WARNING] File not found: {file_path}")
            continue

        with open(file_path, encoding="utf-8") as file:
            src = file.read()

        soup = BeautifulSoup(src, "lxml")
        items_cards = soup.find_all("div", class_="item relative")

        for item in items_cards:
            t = item.find("h2", class_="title").text
            tl = 'https://www.dzo.com.ua' + item.find('a', class_="globalLink").get('href')
            cpv = item.find("div", class_="cpv cd").text

            v = item.find("div", class_="newkvaziName clear").find_next('span').find_next('span').text.rstrip(' Гривня')
            a = item.find("div", class_="newauction").find_next('span').find_next('span').text
            s = item.find("div", class_="newstatus").find_next('span').find_next('span').text
            t_id = item.find("div", class_="cd newtenderId").find_next('span').find_next('span').text
            cdb = item.find("div", class_="cd newtenderMethod CDB_Number").find_next('span').find_next('span').text
            t_method = item.find("div", class_="cd newtenderMethod").find_next('span').find_next('span').text

            #print(f"Tender: {t} - URL: {tl} - INFO: {cpv} - Очікувана вартість закупівлі: {v} - Дата оголошення процедури: {a} - Поточний статус процедури: {s} - Ідентифікатор закупівлі: {t_id} - Організатор закупівлі: {cdb} - Процедура закупівлі: {t_method}")
            data.append(
                {
                    "t": t,
                    "tl": tl,
                    "cpv": cpv,
                    "v": v,
                    "a": a,
                    "s": s,
                    "t_id": t_id,
                    "cdb": cdb,
                    "t_method": t_method,
                    "edr_id": code_e  # Добавим edr_id в данные
                }
            )
            with open(csv_filename, "a", encoding="utf-8") as csv_file:
                csv_writer = csv.writer(csv_file)

                csv_writer.writerow(
                    (
                        t,
                        tl,
                        cpv,
                        v,
                        a,
                        s,
                        t_id,
                        cdb,
                        t_method,
                        code_e  # Добавим edr_id в CSV
                    )
                )
        print(f"[INFO] Обработана страница {page}/{pages_count}")
        time.sleep(3)

    with open(json_filename, "w", encoding="utf-8") as json_file:
        json.dump(data, json_file, indent=4, ensure_ascii=False)
    print("[INFO] Все итерации завершены")

def main():
   # pages_count =  get_all_pages()
   # collect_data(pages_count=pages_count)
    code_e = read_csv_file('edrpo.csv')
    get_all_pages()
    collect_data(pages_count=pages_count)

if __name__ == '__main__':
    main()